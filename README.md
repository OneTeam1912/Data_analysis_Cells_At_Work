# Data_analysis_Cells_At_Work
我们的目标是爬去b站上《工作细胞》的所有短评进行分析，用数据说明为什么这部动漫会如此受欢迎。
首先要做的是爬去b站的所有短评，包括评论用户名、评论时间、星级（评分）、评论内容、点赞数等内容。爬的过程写了很久，b站短评不需要登录直接就可以爬，刚开始
用Selenium+xpath定位爬。但b站短评用这种方法并不好处理。网站每次最多显示20条，所以刚开始用了每次爬完之后将定位到当前爬的位置的方法，这样定位到当前加载
的最后一条时，就会加载之后的20条短评。

逻辑上是解决了这个问题，但真的爬的时候就出现了问题，一个是爬的慢，20条需要十来秒的样子，这个没关系，大不了爬几个小时，但问题是辛辛苦苦爬了两千多条后，
就自动断了，不知道是什么原因，虽然之前爬的数据都存下来了，但没法接着断开但地方接着爬，又要重新开始，还不知道会不会又突然断，所以用这种方法基本就无解了。
代码在spyder_code.py文件中。

这种方法爬去失败之后，一直不知道该怎么处理，刚好最近看到网上有大神爬猫眼评论断文章，依葫芦画瓢尝试了一下，居然成功了，而且爬断速度也很快，十来分钟就全
爬完了，思路是找到评论对应断JSon文件，然后获取JSon中的数据，过程如下。在Google浏览器中按F12打开卡发者工具后，选择Network
往下滑动，会发现过一段时间，会出现一个fetch，右键打开后发现，里面就是20条记录，有所有我们需要的内容，json格式。
所以现在需要做的就是去找这些json文件的路径的规律。多看几条之后，就发现了规律：
第一个json：
https://bangumi.bilibili.com/review/web_api/short/list?media_id=102392&folded=0&page_size=20&sort=0

第二个json：
https://bangumi.bilibili.com/review/web_api/short/list?media_id=102392&folded=0&page_size=20&sort=0&cursor=76553500953424

第三个json：
https://bangumi.bilibili.com/review/web_api/short/list?media_id=102392&folded=0&page_size=20&sort=0&cursor=76549205971454

显然所有的json路径的前半部分都是一样，都是在第一条json之后加上不同的cursor = xxxxx，所以只要能找到cursor值的规律，就可以用循环的办法，
爬完所有的json，这个值看上去没什么规律，最后发现，每一个json路径中cursor值就藏在前一个json的最后一条评论中

在python中可以直接把json转成字典，cursor值就是最后一条评论中键cursor的值，简直不要太容易。

所以爬的思路就很清晰了，从一个json开始，爬完20条评论后，获取最后一个评论中的cursor值，更改路径之后获取第二个json，重复上面的过程，直
到爬完所有的json。

至于如何知道爬完了所有json，也很容易，每个json中一个total键，表示了当前一共有多少条评论，所以只需要写一个while循环，当爬到的评论
数达到total值时停止。

爬的过程中还发现，有些json中的评论数不够20条，如果每次用20去定位，中间会报错停止，需要注意一下。所以又加了一行代码，每次获得json后，通
过len()函数得到当前json中一共包含多少条评论，cursor在最后一个评论中。

需要说明的地方，一个是liked按照字面意思应该是用户的点赞数,但爬完才发现全是0，没有用。另一个是关于时间，里面有ctime和mtime两个跟时
间有关的值，看了几个，基本都是一样的，有个别不太一样，差的不多，就只取了ctime，我猜可能一个是点击进去的时间，一个是评论提交时间，但没法
验证，就随便取一个算了，ctime的编码很奇怪，比如某一个是ctime = 1540001677，渣渣之前没有见过这种编码方式，请教了大佬之后知
道，这个是Linux系统上的时间表示方式，是1970年1月1日0时0分0秒到当时时点的秒数，python中可以直接用time.gmtime()函数转化成年
月日小时分钟秒的格式。还有last_ep_index里面
存的是用户当前的看剧状态，比如看至第13话，第6话之类的，但后来发现很不准，绝大多数用户没有last_ep_index值，所以也没有分析这个变量。
